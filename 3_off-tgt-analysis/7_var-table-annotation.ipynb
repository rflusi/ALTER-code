{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80845f8e",
   "metadata": {},
   "source": [
    "- Purpose:\n",
    "    - annotate the table of variant calls from the GATK results and prepare it for downstream analysis\n",
    "    - infer strand and adjust sequences based on coding strand\n",
    "    - add per sample base data for reference and alt bases and convert GT calls to binary\n",
    "    - cross reference variant entries to ClinVar\n",
    "    - calculate %_ref for all samples in all entries\n",
    "    - calculate %_snp if the target SNP is detected in a sample in a given entry\n",
    "- Outputs\n",
    "    1. a compressed tsv of the fully annotated variant table\n",
    "    2. compressed tsvs with\n",
    "        - binary genotype counts for each sample\n",
    "        - stand assignment counts \n",
    "        - histogram tables for %_ref, %_snp, read depth, QD, and qual\n",
    "    3. an excel sheet compiling all data from 2\n",
    "    4. a tsv file of the entry for the editing target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# import statements #\n",
    "#####################\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from Bio.Seq import Seq\n",
    "import pysam\n",
    "import math\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# User-Defined Variables #\n",
    "##########################\n",
    "# - define all variables below with paths to the required files\n",
    "# - this should be the only cell that requires modification\n",
    "\n",
    "# full path to tsv output from GATK analysis\n",
    "tsv_path =''\n",
    "# full path to gtf for the reference genome\n",
    "gtfgz_path = ''\n",
    "# full path to clinvar vcf file\n",
    "clinvar_vcf_path=''\n",
    "\n",
    "# these must match the sample identifiers in the column titles of the variants tsv\n",
    "samples = [\n",
    "    'R9761',\n",
    "    'R9762',\n",
    "    'R9763',\n",
    "    'R9764',\n",
    "    'R9765',\n",
    "    'R9766',\n",
    "]\n",
    "\n",
    "# target SNPs as a list of stings for example we are interested in C-to-T (C-to-U) so ['CT']\n",
    "target_snps = ['CT']\n",
    "# (chrom, pos) in the format of the genomic reference\n",
    "target_edit = ('chr15', 64162933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically defined variables\n",
    "work_path = os.path.split(tsv_path)[0]\n",
    "out_path = os.path.join(work_path, 'init-processing')\n",
    "\n",
    "# make directory that will hold all results\n",
    "os.makedirs(out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "def import_VariantsToTable_tsv(path, samples):\n",
    "    ###################################################################################################\n",
    "    # Purpose: import variant table output from GATK, standardize datatypes and adjust column headers #\n",
    "    # Inputs: 1. path - string, path of GATK output tsv file                                          #\n",
    "    #         2. samples - list of strings with sample names                                          #\n",
    "    # Output: formatted pandas dataframe created from the tsv data                                    #\n",
    "    ###################################################################################################\n",
    "\n",
    "    dtype_dict = {'CHROM':'str', 'POS':'int', 'REF':'str', 'ALT':'str', 'QUAL':'float', 'FILTER':'str'}\n",
    "    for sample in samples:\n",
    "        dtype_dict[f'{sample}.GT'] = 'str'\n",
    "        dtype_dict[f'{sample}.AD'] = 'str'\n",
    "        dtype_dict[f'{sample}.DP'] = 'float'\n",
    "        dtype_dict[f'{sample}.GQ'] = 'float'\n",
    "    df = pd.read_csv(path, sep='\\t', dtype=dtype_dict, compression='infer')\n",
    "    rename_dict = {}\n",
    "    for column in df.columns:\n",
    "        if '.' not in column:\n",
    "            rename_dict[column] = column.lower()\n",
    "    df = df.rename(columns=rename_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "var_df = import_VariantsToTable_tsv(tsv_path, samples=samples)\n",
    "\n",
    "# convert nan genotypes to a string representation\n",
    "gt_cols = [f'{sample}.GT' for sample in samples]\n",
    "for col in gt_cols:\n",
    "    var_df[col] = var_df[col].apply(lambda x: './.' if pd.isna(x) else x)\n",
    "\n",
    "# display dataframe info\n",
    "total_count = len(var_df)\n",
    "print(f'Total Entries:\\t{total_count}\\n')\n",
    "display(var_df.head())\n",
    "\n",
    "dtypes_df = pd.DataFrame({'dtype':['object', 'str', 'float', 'int']})\n",
    "for column in var_df.columns:\n",
    "    dtype_counts = [0 for i in range(len(dtypes_df))]\n",
    "    dtype_totals = var_df[column].apply(lambda x: str(type(x))).value_counts()\n",
    "    for counted_dtype in dtype_totals.index:\n",
    "        for dtype_idx, dtype_row in dtypes_df.iterrows():\n",
    "            target_dtype = dtype_row['dtype']\n",
    "            if target_dtype in counted_dtype:\n",
    "                dtype_counts[dtype_idx] += dtype_totals[counted_dtype]\n",
    "    dtypes_df_col = pd.DataFrame({f'{column}:{var_df[column].dtype}':dtype_counts})\n",
    "    dtypes_df = pd.concat([dtypes_df, dtypes_df_col], axis=1)\n",
    "display(dtypes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34306a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer entry strand and correct sequences to reflect coding strand\n",
    "\n",
    "def strand_id(var_row, gtfgz, feat_priority):\n",
    "    ###################################################################################################\n",
    "    # Purpose: take variant information from a row of the variant dataframe and obtain relevant       #\n",
    "    #          feature information from the reference gtf                                             #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. gtfgz - gtfgz file imported with pysam.TabixFile()                                   #\n",
    "    #         3. feat_priority - a list of feature types in priority order for assigning strands (ie. #\n",
    "    #                            if feat_priority=['exon','transcript'] and a variant location has    #\n",
    "    #                            transcript features in both strands but exon features in only one,   #\n",
    "    #                            the exon strand will be assigned)                                    #\n",
    "    # Output: values for strand, gene_name, gene_id, transcript_id, exon_id, and warning notes        #\n",
    "    ###################################################################################################\n",
    "    \n",
    "    # the warning notes will be kept as a list then combined into a string at the end\n",
    "    warning_str = [var_row['warnings']] if (var_row['warnings'] != '') else []\n",
    "    try:\n",
    "        chrom = var_row['chrom']\n",
    "        query_start = var_row['pos']\n",
    "        query_end = var_row['pos'] + 1 \n",
    "\n",
    "        # iterate through feature types in priority order, look for features at the variant position\n",
    "        # if features of that type are found in only one strand, that strand will be assigned if they\n",
    "        # are in both strands it will be assigned 'ambiguous'. Other information is also obtaned accordingly\n",
    "        # id and named info are stored as matched comma-separated values so that multiple hits can be accurately\n",
    "        # assessed later\n",
    "        for target_feature in feat_priority:\n",
    "            target_rows = []\n",
    "            for gtfgz_row in gtfgz.fetch(chrom, query_start, query_end):\n",
    "                gtfgz_row_list = gtfgz_row.strip().split('\\t')\n",
    "                feat_type = gtfgz_row_list[2].strip()\n",
    "                if feat_type == target_feature:\n",
    "                    target_rows.append(gtfgz_row_list)\n",
    "        \n",
    "            if len(target_rows) > 0:\n",
    "                if len(target_rows) > 1:\n",
    "                    warning_str.append(f'overlapping {target_feature}s')\n",
    "                    \n",
    "                    strand_set = list(set(gftgz_row[6] for gftgz_row in target_rows))\n",
    "                    if len(strand_set) == 0:\n",
    "                        print(f'[WARNING] {target_feature}s identified but no strands identified')\n",
    "                    elif len(strand_set) > 1:\n",
    "                        strand = 'ambiguous'\n",
    "                    else:\n",
    "                        strand = strand_set[0]\n",
    "                else:\n",
    "                    strand_set = list(set(gftgz_row[6] for gftgz_row in target_rows))\n",
    "                    strand = strand_set[0]\n",
    "\n",
    "                if target_feature in ['exon']:\n",
    "                    exon_id = ','.join(list((gftgz_row[8].split('exon_id')[1].split('\"')[1].strip()) for gftgz_row in target_rows))\n",
    "                else:\n",
    "                    exon_id = ''\n",
    "\n",
    "                if target_feature in ['exon', 'transcript']:\n",
    "                    transcript_id = ','.join(list((gftgz_row[8].split('transcript_id')[1].split('\"')[1].strip()) for gftgz_row in target_rows))\n",
    "                else:\n",
    "                    transcript_id = ''\n",
    "                    \n",
    "\n",
    "                gene_name = ','.join(list((gftgz_row[8].split('gene_name')[1].split('\"')[1].strip()) for gftgz_row in target_rows))\n",
    "                gene_id = ','.join(list((gftgz_row[8].split('gene_id')[1].split('\"')[1].strip()) for gftgz_row in target_rows))\n",
    "\n",
    "                return strand, gene_name, gene_id, transcript_id, exon_id, ','.join(warning_str)\n",
    "\n",
    "            else:\n",
    "                warning_str.append(f'no {target_feature}s')\n",
    "\n",
    "        warning_str.append('feature search failed')\n",
    "        return *['' for i in range(5)], ','.join(warning_str)\n",
    "    except ValueError:\n",
    "        warning_str.append('no features')\n",
    "        return *['' for i in range(5)], ','.join(warning_str)\n",
    "\n",
    "\n",
    "def strand_sequences(var_row, samples):\n",
    "    ###################################################################################################\n",
    "    # Purpose: adjust variant and GT sequences if the coding strand is -                              #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. samples - a list of sample names                                                     #\n",
    "    # Output: values for ref, alt and all GT columns with appropriate sequences                       #\n",
    "    ###################################################################################################\n",
    "    \n",
    "    # if the coding strand is + simply return the existing sequence data unchanged\n",
    "    if var_row['strand'] != '-':\n",
    "        return var_row['ref'], var_row['alt'], *[var_row[f'{sample}.GT'] for sample in samples]\n",
    "    elif var_row['strand'] == '-':\n",
    "        init_ref = var_row['ref']\n",
    "        init_alt = var_row['alt']\n",
    "\n",
    "        # get the reverse complement sequences for ref and alt, check for multiple values in each and handle that if necessary\n",
    "        rt_list = []\n",
    "        for alt in [init_ref, init_alt]: # I know the variable naming here is confusing, just roll with it\n",
    "            if ('|' not in alt) and (',' not in alt):\n",
    "                alt_rc = str(Seq(alt).reverse_complement()).strip()\n",
    "            elif ('|' in alt) and (',' in alt):\n",
    "                alt_list = alt.split('|')\n",
    "                alt_list = [single_alt.strip() for single_alt in alt_list]\n",
    "\n",
    "                for i in range(len(alt_list)):\n",
    "                    if ',' in alt_list[i]:\n",
    "                        alt_sublist = alt_list[i].split(',')\n",
    "                        alt_sublist = [single_alt.strip() for single_alt in alt_sublist]\n",
    "                        new_sublist = []\n",
    "                        for single_alt in alt_sublist:\n",
    "                            new_sublist.append(str(Seq(single_alt).reverse_complement()).strip())\n",
    "                        new_sublist = ','.join(new_sublist)\n",
    "                        alt_list[i] = new_sublist\n",
    "                    else:\n",
    "                        alt_list[i] = str(Seq(alt_list[i]).reverse_complement().strip())\n",
    "\n",
    "                alt_rc = '|'.join(alt_list)\n",
    "            else:\n",
    "                alt_delim = '|' if ('|' in alt) else ','\n",
    "\n",
    "                alt_list = alt.split(alt_delim)\n",
    "                alt_list = [single_alt.strip() for single_alt in alt_list]\n",
    "                alt_list = [(str(Seq(single_alt).reverse_complement().strip())) for single_alt in alt_list]\n",
    "                \n",
    "                alt_rc = alt_delim.join(alt_list)\n",
    "                \n",
    "            rt_list.append(alt_rc)\n",
    "\n",
    "        # get the reverse complement sequences for GT values\n",
    "        gt_rcs = []\n",
    "        for sample in samples:\n",
    "            gt = var_row[f'{sample}.GT']\n",
    "            if not pd.isna(gt):    \n",
    "                if '/' in gt:\n",
    "                    gt_delim = '/'\n",
    "                    alls = gt.split(gt_delim)\n",
    "                elif '|' in gt:\n",
    "                    gt_delim = '|'\n",
    "                    alls = gt.split(gt_delim)\n",
    "                else:\n",
    "                    print(gt)\n",
    "                gt_rc = []\n",
    "                for all in alls:\n",
    "                    gt_rc.append(str(Seq(all).reverse_complement().strip()))\n",
    "                gt_rc = gt_delim.join(gt_rc)\n",
    "                gt_rcs.append(gt_rc)\n",
    "            else:\n",
    "                gt_rc = './.'\n",
    "                gt_rcs.append(gt_rc)\n",
    "            \n",
    "        return rt_list[0], rt_list[1], *gt_rcs\n",
    "\n",
    "\n",
    "\n",
    "init_dot_gt_counts = [(var_df[f'{sample}.GT'].value_counts()['./.'] if ('./.' in var_df[f'{sample}.GT'].value_counts().index) else 0) for sample in samples]\n",
    "\n",
    "gtfgz = pysam.TabixFile(gtfgz_path)\n",
    "\n",
    "# initialize feature annotation columns\n",
    "feature_cols = ['strand', 'gene_name', 'gene_id', 'transcript_id', 'exon_id', 'warnings']\n",
    "for feature_col in feature_cols:\n",
    "    var_df[feature_col] = ''\n",
    "\n",
    "# add var_df columns with strand and annotation data\n",
    "feat_priority = ['exon', 'transcript', 'gene']\n",
    "var_df[feature_cols] = var_df[['chrom', 'pos', 'warnings']].apply(strand_id, gtfgz=gtfgz, feat_priority=feat_priority, axis=1, result_type='expand')\n",
    "\n",
    "# if the coding strand is assigned - adjust the sequence identity of the variant and sample genotypes\n",
    "seq_cols = ['ref', 'alt'] + [f'{sample}.GT' for sample in samples]\n",
    "var_df[seq_cols] = var_df[(['ref', 'alt', 'strand']+ [f'{sample}.GT' for sample in samples])].apply(strand_sequences, axis=1, samples=samples, result_type='expand')\n",
    "\n",
    "# display the number of uncalled genotypes for each sample and the new columns\n",
    "for i in range(len(samples)):\n",
    "    print(f'Initial {samples[i]} ./. GTs:{init_dot_gt_counts[i]}')\n",
    "print('\\n')\n",
    "display_cols = ['chrom', 'pos'] + feature_cols\n",
    "display(var_df[display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all feature id columns have been populated with matched lists\n",
    "# if there are mismatches the rows will be printed below\n",
    "\n",
    "def mask_func(var_row):\n",
    "    ###################################################################################################\n",
    "    # Purpose: check that feature id lists are the same length and therefor corresponding             #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    # Output: True if the lengths are matched and false if they are not, used to build a boolean mask #\n",
    "    ###################################################################################################\n",
    "    count_list = []\n",
    "    if var_row['gene_id'] != '':\n",
    "        count_list.append(len(var_row['gene_id'].split(',')))\n",
    "    if var_row['transcript_id'] != '':\n",
    "        count_list.append(len(var_row['transcript_id'].split(',')))\n",
    "    if var_row['exon_id'] != '':\n",
    "        count_list.append(len(var_row['exon_id'].split(',')))\n",
    "\n",
    "    counts_equal = True\n",
    "    for i in range(1,len(count_list)):\n",
    "        if count_list[i-1] != count_list[i]:\n",
    "            counts_equal = False\n",
    "\n",
    "    if not counts_equal:\n",
    "        print(f'\\n{var_row['gene_id'].split(',')}\\t{len(var_row['gene_id'].split(','))}')\n",
    "        print(f'\\n{var_row['transcript_id'].split(',')}\\t{len(var_row['transcript_id'].split(','))}')\n",
    "        print(f'\\n{var_row['exon_id'].split(',')}\\t{len(var_row['exon_id'].split(','))}\\n')\n",
    "\n",
    "    return counts_equal\n",
    "\n",
    "display_mask = var_df[['gene_id','transcript_id','exon_id']].apply(mask_func, axis=1)\n",
    "display(var_df.loc[~display_mask, display_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display stranding information for the dataset\n",
    "\n",
    "print(f'Total Hits: {len(var_df)}\\n')\n",
    "value_counts = var_df['strand'].value_counts()\n",
    "strand_counts = pd.DataFrame({\n",
    "    'strand':value_counts.index,\n",
    "    'count':value_counts.values\n",
    "})\n",
    "display(strand_counts)\n",
    "\n",
    "value_counts = pd.Series(zip(var_df['strand'], var_df['warnings'])).value_counts()\n",
    "strand_warning_counts = pd.DataFrame({\n",
    "    'strand, warnings':value_counts.index,\n",
    "    'count':value_counts.values\n",
    "})\n",
    "display(strand_warning_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a510ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate per sample nucleotides\n",
    "def id_sample_nts(var_row, sample, all_samples):\n",
    "    ###################################################################################################\n",
    "    # Purpose: add sample specific ref and alt columns                                                #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. sample - the relevant sample                                                         #\n",
    "    #         3. all_samples - a list of all samples                                                  #\n",
    "    # Output: for each sample, values for the reference nucleotide and genotype nucleotides 1 and 2   #\n",
    "    ###################################################################################################\n",
    "    if var_row[f'{sample}.GT'].strip() == './.':\n",
    "        return '','',''\n",
    "    else:\n",
    "        init_ref = var_row['ref'].strip()\n",
    "        init_gt = var_row[f'{sample}.GT'].strip()\n",
    "        \n",
    "        gt_delim = '|' if ('|' in init_gt) else '/'\n",
    "        sample_alls = init_gt.split(gt_delim)\n",
    "        sample_alls = [sample_all.strip() for sample_all in sample_alls]\n",
    "\n",
    "        if (',' not in init_ref) and ('|' not in init_ref):\n",
    "            return init_ref, sample_alls[0], sample_alls[1]\n",
    "        elif ('|' in init_ref) or (',' in init_ref):\n",
    "            grouped_refs = init_ref.split('|')\n",
    "            grouped_refs = [refs.strip() for refs in grouped_refs]\n",
    "            grouped_refs = [refs.split(',') for refs in grouped_refs]\n",
    "\n",
    "            if len(grouped_refs) == len(all_samples):\n",
    "                sample_ref = grouped_refs[all_samples.index(sample)]\n",
    "                sample_ref = ','.join(sample_ref)\n",
    "                return sample_ref, sample_alls[0], sample_alls[1]\n",
    "            \n",
    "            refs_list = []\n",
    "            for ref_list in grouped_refs:\n",
    "                for ref in ref_list:\n",
    "                    refs_list.append(ref.strip())\n",
    "            \n",
    "            init_alt = var_row['alt'].strip()\n",
    "            alts_list = init_alt.split('|')\n",
    "            alts_list = [alt.strip() for alt in alts_list]\n",
    "            alts_list = ','.join(alts_list)\n",
    "            alts_list = alts_list.split(',')\n",
    "            alts_list = [alt.strip() for alt in alts_list]\n",
    "            \n",
    "            ref_alt_overlap = set(refs_list).intersection(set(alts_list))\n",
    "            if len(ref_alt_overlap) > 0:\n",
    "                var_samples = []\n",
    "                for samp in all_samples:\n",
    "                    if var_row[f'{samp}.GT'] != './.':\n",
    "                        var_samples.append(samp)\n",
    "\n",
    "                if len(grouped_refs) == len(var_samples):\n",
    "                    sample_ref = grouped_refs[var_samples.index(sample)]\n",
    "                    sample_ref = ','.join(sample_ref)\n",
    "                elif (sample_alls[0] not in ref_alt_overlap) or (sample_alls[1] not in ref_alt_overlap):\n",
    "                    sample_ref = ''\n",
    "                    for sample_all in sample_alls:\n",
    "                        if sample_all not in ref_alt_overlap:\n",
    "                            if sample_all in refs_list:\n",
    "                                if (sample_ref != '') and (sample_ref != sample_all):\n",
    "                                    print(f'[WARNING] Conflicting refs:\\tindex:{var_row.name}ref:{init_ref}\\talt:{init_alt}\\tGT:{init_gt}')\n",
    "                                    return '', sample_alls[0], sample_alls[1]\n",
    "                                sample_ref = sample_all\n",
    "                else:\n",
    "                    sample_ref = ''\n",
    "\n",
    "            else:\n",
    "                sample_ref = ''\n",
    "                for sample_all in sample_alls:\n",
    "                    if sample_all in refs_list:\n",
    "                        if (sample_ref != '') and (sample_ref != sample_all):\n",
    "                            print(f'[WARNING] Conflicting refs:\\tindex:{var_row.name}ref:{init_ref}\\talt:{init_alt}\\tGT:{init_gt}')\n",
    "                            return '', sample_alls[0], sample_alls[1]\n",
    "                        sample_ref = sample_all\n",
    "            return sample_ref, sample_alls[0], sample_alls[1]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "for sample in samples:\n",
    "    new_nt_cols = [f'{sample}_ref', f'{sample}_all_1', f'{sample}_all_2']\n",
    "    for new_nt_col in new_nt_cols:\n",
    "        var_df[new_nt_col] = ''\n",
    "    var_df[new_nt_cols] = var_df[['ref', 'alt'] + [f'{sample}.GT' for sample in samples]].apply(id_sample_nts, sample=sample, axis=1, result_type='expand', all_samples=samples)\n",
    "\n",
    "# for print output\n",
    "display_cols = ['ref', 'alt']\n",
    "for sample in samples:\n",
    "    display_cols = display_cols + [f'{sample}.GT', f'{sample}_ref', f'{sample}_all_1', f'{sample}_all_2'] \n",
    "print('var_df')\n",
    "display(var_df[display_cols].head())\n",
    "\n",
    "# non ./. genotyped entries without nucleotide assignments will be displayed at the bottom of the output, the table should be empty\n",
    "sample_masks = []\n",
    "entry_count = 0\n",
    "for sample in samples:\n",
    "    mask = ((var_df[f'{sample}.GT'] != './.') & (var_df[f'{sample}_ref'] == ''))\n",
    "    sample_masks.append(mask)\n",
    "    entry_count += len(var_df[mask])\n",
    "print(f'Non ./. Sample x Entries that are Unassigned ({entry_count}, {round((entry_count/(len(var_df)*len(samples)))*100,2)}%):')\n",
    "display(var_df.loc[reduce(lambda x,y: (x | y), sample_masks), display_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd954fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross reference variants to ClinVar and add ClinVar information to the table\n",
    "def check_in_clinvar(var_row, clinvar_vcf, samples):\n",
    "    ###################################################################################################\n",
    "    # Purpose: add columns with ClinVar data to each variant entry                                    #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. clinvar_vcf - ClinVar vcf file read with pysam.VariantFile()                         #\n",
    "    #         3. samples - a list of all samples                                                      #\n",
    "    # Output: 1. in_clinvar - boolean of whether the variant appears in clinvar                       #\n",
    "    #         2. clinvar_ids - all clinvar ids associated with the variant                            #\n",
    "    #         3. clinvar_sig - the variant significance as annotated in clinvar                       #\n",
    "    #         4. clinvar_disease - diseases associated with the variant in clinvar                    #\n",
    "    ###################################################################################################\n",
    "\n",
    "    chrom = var_row['chrom']\n",
    "    pos = var_row['pos']\n",
    "    strand = var_row['strand']\n",
    "\n",
    "    in_clinvar = False\n",
    "    clinvar_ids = set()\n",
    "    clinvar_sig = set()\n",
    "    clinvar_disease = []\n",
    "\n",
    "    for sample in samples:\n",
    "        ref = var_row[f'{sample}_ref']\n",
    "        sample_alts = set()\n",
    "        for sample_all in [var_row[f'{sample}_all_1'], var_row[f'{sample}_all_2']]:\n",
    "            if (sample_all != ref):\n",
    "                sample_alts.add(sample_all)\n",
    "\n",
    "        # clinvar entries corespond to the + strand so the nucleotide identities need to be reversed if the variant was assigned to the - strand\n",
    "        if strand == '-':\n",
    "            ref = str(Seq(ref).reverse_complement())\n",
    "            sample_alts = set(str(Seq(alt).reverse_complement()) for alt in sample_alts)\n",
    "\n",
    "        # check whether the variant is in clinvar and pull the related clinvar information if so\n",
    "        try:\n",
    "            for clinvar_rec in clinvar_vcf.fetch(chrom, pos-1, pos): # fetch in 0 index vcf is 1 index\n",
    "                if (len(sample_alts) > 0) and (clinvar_rec.alts != None):\n",
    "                    if (clinvar_rec.alleles[0] == ref) and (len(sample_alts.intersection(set(clinvar_rec.alts))) > 0):\n",
    "                        in_clinvar = True\n",
    "                        clinvar_ids.add(clinvar_rec.id)\n",
    "                        if 'CLNSIG' in clinvar_rec.info.keys():\n",
    "                            clinvar_sig.add(','.join(clinvar_rec.info['CLNSIG']))\n",
    "                        if 'CLNDN' in clinvar_rec.info.keys():\n",
    "                            clinvar_disease.append(','.join(clinvar_rec.info['CLNDN']))\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    clinvar_ids = ','.join(clinvar_ids)\n",
    "    clinvar_sig = ','.join(clinvar_sig)\n",
    "    clinvar_disease = ','.join(clinvar_disease)\n",
    "\n",
    "    return in_clinvar, clinvar_ids, clinvar_sig, clinvar_disease\n",
    "\n",
    "\n",
    "\n",
    "clinvar_vcf = pysam.VariantFile(clinvar_vcf_path, 'r')\n",
    "var_df[['in_clinvar', 'clinvar_ids', 'clinvar_sig', 'clinvar_disease']] = var_df.apply(check_in_clinvar, clinvar_vcf=clinvar_vcf, samples=samples, axis=1, result_type='expand')\n",
    "\n",
    "# print aggregated value counts for how many entries appear in ClinVar with a given significance\n",
    "# and the head of the dataframe with new ClinVar columns\n",
    "print(f'Total Entries in ClinVar:\\t{len(var_df[var_df['in_clinvar']])}')\n",
    "display(var_df.loc[var_df['in_clinvar'], 'clinvar_sig'].value_counts())\n",
    "display(var_df.loc[var_df['in_clinvar'], ['chrom', 'pos', 'ref', 'alt', 'clinvar_ids', 'clinvar_sig', 'clinvar_disease']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert GTs\n",
    "def get_bin_gt(var_row, sample):\n",
    "    ###################################################################################################\n",
    "    # Purpose: convert all genotypes to binary genotyping (0 for ref 1 for alt)                       #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. sample - the sample for which the genotype should be converted                       #\n",
    "    # Output: The binary genotype value for the specified sample                                      #\n",
    "    ###################################################################################################\n",
    "    \n",
    "    sample_gt = var_row[f'{sample}.GT'].strip()\n",
    "    if sample_gt == './.':\n",
    "        return './.'\n",
    "    else:\n",
    "        sample_ref = var_row[f'{sample}_ref'].strip()\n",
    "\n",
    "        gt_delim = ''\n",
    "        for delim in ['/', '|']:\n",
    "            gt_delim = delim if (delim in sample_gt) else gt_delim\n",
    "        \n",
    "        sample_alls = sample_gt.split(gt_delim)\n",
    "        sample_alls = [sample_all.strip() for sample_all in sample_alls]\n",
    "        bin_gt = ['.', '.']\n",
    "\n",
    "        for i in range(len(sample_alls)):\n",
    "            bin_gt[i] = str(0) if (sample_alls[i] == sample_ref) else str(1)\n",
    "        \n",
    "        return gt_delim.join(bin_gt)\n",
    "\n",
    "def get_value_count_col(count_index, value_col_name, target_col, df_slice):\n",
    "    ###################################################################################################\n",
    "    # Purpose: return a 1 column dataframe that can be used to build a value count table across       #\n",
    "    #           multiple columns with the same set of values                                          #\n",
    "    # Inputs: 1. count_idex - list of values to be counted                                            #\n",
    "    #         2. value_col_name - column header for the output column                                 #\n",
    "    #         3. target_col - name of the column whose values will be counted                         #\n",
    "    #         4. df_slice - dataframe with the target column                                          #\n",
    "    # Output: A dataframe with one column that is titled value_col_name and contains counts, in order #\n",
    "    #         for the values in count_index                                                           #\n",
    "    ###################################################################################################\n",
    "\n",
    "    df_counts = df_slice[target_col].value_counts()\n",
    "    df_count_list = []\n",
    "\n",
    "    for count_idx in count_index:\n",
    "        if count_idx in df_counts.index:\n",
    "            df_count_list.append(int(df_counts[count_idx]))\n",
    "        else:\n",
    "            df_count_list.append(0)\n",
    "            \n",
    "    return pd.DataFrame({value_col_name:df_count_list})\n",
    "\n",
    "for sample in samples:\n",
    "    gt_col = f'{sample}.GT'\n",
    "    input_cols = [gt_col, f'{sample}_ref']\n",
    "    var_df[gt_col] = var_df[input_cols].apply(get_bin_gt, sample=sample, axis=1)\n",
    "\n",
    "# display aggregate binary GT counts\n",
    "gt_counts_df = pd.DataFrame({'binary_gt':['1/1', '1/0', '0/1', '0/0', './.']})\n",
    "for sample in samples:\n",
    "    gt_col = f'{sample}.GT'\n",
    "    simple_gt_col = pd.DataFrame(var_df[gt_col].apply(lambda x: x.replace('|', '/')))\n",
    "    val_count_col = get_value_count_col(value_col_name=gt_col, count_index=list(gt_counts_df['binary_gt']), target_col=gt_col, df_slice=simple_gt_col)\n",
    "    gt_counts_df = pd.concat([gt_counts_df, val_count_col], axis=1)\n",
    "display(gt_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % ref values for each sample in all entries and %snp values for samples matching the target snp\n",
    "def calc_pct_ref(ad_str):\n",
    "    ###################################################################################################\n",
    "    # Purpose: calculate the % of reads matching the reference nucleotide for a given AD value        #\n",
    "    # Inputs: 1. ad_str - an AD value                                                                 #\n",
    "    # Output: a %_ref value which is nan if the ad value is also nan                                  #\n",
    "    ###################################################################################################\n",
    "    \n",
    "    if not pd.isna(ad_str):\n",
    "        ads_list = ad_str.strip().split(',')\n",
    "        ads_list = [int(var_ad) for var_ad in ads_list]\n",
    "        total_ad = sum(ads_list)\n",
    "        if total_ad > 0:\n",
    "            ref_ad = ads_list[0]\n",
    "            pct_ref = round(float((ref_ad/total_ad)*100), 2)\n",
    "        else:\n",
    "            pct_ref = np.nan\n",
    "\n",
    "        return pct_ref\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def calc_pct_snp(var_row, sample, target_snps):\n",
    "    ###################################################################################################\n",
    "    # Purpose: calculate the % of reads matching the target SNP if the sample shows the target SNP    #\n",
    "    # Inputs: 1. var_row - a subset of the dataframe row passed with pd.apply(axis=1)                 #\n",
    "    #         2. sample - the sample for which to target %_snp                                        #\n",
    "    #         3. target_snps - a list of the target SNPs                                              #\n",
    "    # Output: a %_snp value, this will be nan if the sample does not have a variant that matches a    #\n",
    "    #         target SNP                                                                              #\n",
    "    ###################################################################################################\n",
    "\n",
    "    ad_str = var_row[ad_col]\n",
    "    sample_ref = var_row[f'{sample}_ref']\n",
    "    if (not pd.isna(ad_str)) and (sample_ref != ''):\n",
    "        all_1 = var_row[f'{sample}_all_1']\n",
    "        all_2 = var_row[f'{sample}_all_2']\n",
    "\n",
    "        for target_snp in target_snps:\n",
    "            if ((sample_ref + all_1) == target_snp) or ((sample_ref + all_2) == target_snp):\n",
    "                ads_list = ad_str.strip().split(',')\n",
    "                ads_list = [int(var_ad) for var_ad in ads_list]\n",
    "                total_ad = sum(ads_list)\n",
    "\n",
    "                if total_ad > 0:\n",
    "                    if len(ads_list) == 2:\n",
    "                        pct_snp = round(float((ads_list[1]/total_ad)*100),2)\n",
    "                    # if there are more than 2 alt nucleotides for a given sample it is ignored, this generally occurs for a miniscule fraction of sample entries\n",
    "                    elif len(ads_list) > 3:\n",
    "                        pct_snp = np.nan\n",
    "                    # if there are 3 ADs check whether the alts match or not and calculate accordingly\n",
    "                    else:\n",
    "                        if all_1 == all_2:\n",
    "                            pct_snp = round(float((sum(ads_list[1:])/total_ad)*100),2)\n",
    "                        else:\n",
    "                            pct_snp = round(float((ads_list[([all_1, all_2].index(target_snp[1]) + 1)]/total_ad)*100),2)\n",
    "                else:\n",
    "                    pct_snp = np.nan\n",
    "            else:\n",
    "                pct_snp = np.nan\n",
    "    else:\n",
    "        pct_snp = np.nan\n",
    "\n",
    "    return pct_snp\n",
    "\n",
    "\n",
    "for sample in samples:\n",
    "    ad_col = f'{sample}.AD'\n",
    "    var_df[f'{sample}_pct_ref'] = var_df[ad_col].apply(calc_pct_ref)\n",
    "    input_cols = [ad_col, f'{sample}_ref', f'{sample}_all_1', f'{sample}_all_2']\n",
    "    var_df[f'{sample}_pct_snp'] = var_df[input_cols].apply(calc_pct_snp, sample=sample, target_snps=target_snps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey and check %snp calculations\n",
    "\n",
    "# print total var_df entries and entries*samples and counts for entries where all or some samples have %_snp values\n",
    "total_count = len(var_df) * len(samples)\n",
    "pct_snp_masks = []\n",
    "pct_snp_count = 0\n",
    "for sample in samples:\n",
    "    mask = var_df[f'{sample}_pct_snp'].apply(lambda x: not pd.isna(x))\n",
    "    pct_snp_masks.append(mask)\n",
    "    pct_snp_count += mask.value_counts()[True]\n",
    "\n",
    "all_pct_snp_mask = reduce(lambda x,y: (x&y), pct_snp_masks)\n",
    "any_pct_snp_mask = reduce(lambda x,y: (x|y), pct_snp_masks)\n",
    "\n",
    "print(f'Total Entries * Samples: {total_count}')\n",
    "print(f'Total Assigned pct_snp: {pct_snp_count}\\n')\n",
    "print(f'Total Entries: {len(var_df)}')\n",
    "print(f'Entries with all samples pct_snp: {len(var_df[all_pct_snp_mask])}')\n",
    "print(f'Entries with not all samples pct_snp: {len(var_df[any_pct_snp_mask]) - len(var_df[all_pct_snp_mask])}\\n')\n",
    "\n",
    "# print some var_df rows with all samples assigned %_snp and some var_df rows with only some samples assigned %_snp for visual inspection\n",
    "display_cols = ['chrom', 'pos', 'ref', 'alt']\n",
    "for sample in samples:\n",
    "    display_cols = display_cols + [f'{sample}.GT', f'{sample}.AD', f'{sample}_ref', f'{sample}_all_1', f'{sample}_all_2', f'{sample}_pct_snp']\n",
    "print('All samples assigned pct_snp:')\n",
    "display(var_df.loc[all_pct_snp_mask, display_cols]) \n",
    "print('Some samples assigned pct_snp:')\n",
    "display(var_df.loc[any_pct_snp_mask, display_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_table(var_df, target_cols, bucket_lims, mode):\n",
    "    ###################################################################################################\n",
    "    # Purpose: create a histogram table of values in multiple dataframe columns                       #\n",
    "    # Inputs: 1. var_df - the dataframe holding variant data                                          #\n",
    "    #         2. target_cols - the columns whose values should be counted for the table               #\n",
    "    #         3. bucket_lims - a list of numerical limits for the histogram buckets                   #\n",
    "    #         4. mode - if mode == 'log' the beckets will be made on log scale and values will be     #\n",
    "    #                   incremented by 1 so that values of 0 are on scale                             #\n",
    "    # Output: a dataframe containing the histogram table                                              #\n",
    "    ###################################################################################################\n",
    "    if ('log' in mode) and bucket_lims[0] == 0:\n",
    "        bucket_lims[0] = 1\n",
    "    \n",
    "    buckets = []\n",
    "    for i in range(len(bucket_lims[1:])):\n",
    "        buckets.append(f'{bucket_lims[i]}-{bucket_lims[i+1]}') \n",
    "\n",
    "    hist_df = pd.DataFrame({\n",
    "        f'bucket':buckets\n",
    "    })\n",
    "\n",
    "    for target_col in target_cols:\n",
    "        bucket_counts = []\n",
    "        for i in range(len(buckets)):\n",
    "            bucket_counts.append(0)\n",
    "\n",
    "        out_of_range_low_count = 0\n",
    "        out_of_range_high_count = 0\n",
    "        for var_idx, var_row in tqdm.tqdm(var_df.iterrows(), total=len(var_df)):\n",
    "            metric_val = var_row[target_col]\n",
    "            if not pd.isna(metric_val):\n",
    "                if metric_val >= bucket_lims[-1]:\n",
    "                    metric_val = bucket_lims[-1]*0.999\n",
    "                    out_of_range_high_count += 1\n",
    "                elif metric_val < bucket_lims[0]:\n",
    "                    metric_val = bucket_lims[0]\n",
    "                    out_of_range_low_count += 1\n",
    "\n",
    "                if mode == 'scalar':\n",
    "                    bucket_idx = math.floor(metric_val/(bucket_lims[-1]/len(buckets)))\n",
    "                if 'log' in mode:\n",
    "                    log_n = int(mode.split('log')[1])\n",
    "                    bucket_idx = math.floor(math.log(metric_val, log_n))\n",
    "                try:\n",
    "                    bucket_counts[bucket_idx] += 1\n",
    "                except IndexError:\n",
    "                    print(f'{bucket_lims}\\tMetric:{target_col}\\tValue:{metric_val}\\tBucket:{bucket_idx}')\n",
    "                    \n",
    "        hist_row = pd.DataFrame({f'{target_col}':bucket_counts})\n",
    "        hist_df = pd.concat([hist_df, hist_row], axis=1)\n",
    "    \n",
    "    return hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acff173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram tables for %_ref, %_snp and read depth\n",
    "target_cols = [f'{sample}_pct_ref' for sample in samples]\n",
    "num_buckets = 10\n",
    "bucket_lims = list(range(0,101,int(100/num_buckets)))\n",
    "pct_hist_df = hist_table(mode='scalar', var_df=var_df, target_cols=target_cols, bucket_lims=bucket_lims)\n",
    "\n",
    "target_cols = target_cols + [f'{sample}_pct_snp' for sample in samples]\n",
    "pct_snp_hist_cols = pd.DataFrame()\n",
    "for sample in samples:\n",
    "    mask = var_df[f'{sample}_pct_snp'].apply(lambda x: not pd.isna(x))\n",
    "    new_cols = var_df.loc[mask, [f'{sample}_pct_ref', f'{sample}_pct_snp']]\n",
    "    pct_snp_hist_cols = pd.concat([pct_snp_hist_cols, new_cols], axis=1)\n",
    "pct_snp_hist_df = hist_table(mode='scalar', var_df=pct_snp_hist_cols, target_cols=target_cols, bucket_lims=bucket_lims)\n",
    "\n",
    "target_cols = [f'{sample}.DP' for sample in samples] + [f'{sample}.GQ' for sample in samples] + ['qual']\n",
    "num_buckets = 21\n",
    "bucket_lims = list(2**i for i in range(num_buckets+1))\n",
    "dp_hist_df = hist_table(mode='log2', var_df=var_df, target_cols=target_cols, bucket_lims=bucket_lims)\n",
    "\n",
    "display(pct_hist_df)\n",
    "print('\\n')\n",
    "display(pct_snp_hist_df)\n",
    "print('\\n')\n",
    "display(dp_hist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output var_df as a compressed tsv\n",
    "file_name = os.path.split(tsv_path)[1].split('.tsv')[0]\n",
    "out_tsv_path = os.path.join(out_path, f'{file_name}-proc.tsv.gz')\n",
    "var_df.to_csv(out_tsv_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "# output various metrics tables as compressed tsvs\n",
    "export_dfs = {\n",
    "    'bin-gt-counts':gt_counts_df, \n",
    "    'strand-counts':strand_counts,\n",
    "    'strand-warning-counts':strand_warning_counts,\n",
    "    # 'ref-rc-counts':nt_counts,\n",
    "    # 'gt-rc-counts':gt_rc_counts,\n",
    "    'pct-hist':pct_hist_df,\n",
    "    'pct-snp-hist':pct_snp_hist_df,\n",
    "    'dp-hist':dp_hist_df\n",
    "    }\n",
    "\n",
    "for suffix, export_df in export_dfs.items():\n",
    "    out_tsv_path = os.path.join(out_path, f'{file_name}-{suffix}.tsv.gz')\n",
    "    export_df.to_csv(out_tsv_path, sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "# compile metrics tables into an excel output\n",
    "excel_path = os.path.join(out_path, f'{file_name}-summary-stats.xlsx')\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    for sheet_name, export_df in export_dfs.items():\n",
    "        export_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "# print target edit %_ref and %_snp and output full var_df row for the target edit to a tsv file\n",
    "display_col_list = []\n",
    "for sample in samples:\n",
    "    display_col_list.append(f'{sample}_pct_ref')\n",
    "    display_col_list.append(f'{sample}_pct_snp')\n",
    "\n",
    "display_mask = (var_df['chrom'] == target_edit[0]) & (var_df['pos'] == target_edit[1])\n",
    "display(var_df.loc[display_mask,display_cols])\n",
    "\n",
    "out_tsv_path = os.path.join(out_path, f'tgt-edit.tsv')\n",
    "var_df[display_mask].to_csv(out_tsv_path, sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
