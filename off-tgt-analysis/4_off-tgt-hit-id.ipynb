{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4005c255",
   "metadata": {},
   "source": [
    "- Purpose\n",
    "    - filter annotated variant table for final off-tgt hits\n",
    "    - off target hits defined as passing the following filters:\n",
    "        1. var    - sample was called as a variant by HapplotypeCaller\n",
    "        2. VOI    - sample variant matches the variant of interest\n",
    "        3. DP     - read depth >= 20\n",
    "        4. GQ     - depth by quality >=20 (corresponds to 99% confidence)\n",
    "        5. non-wt - pct_ref in the matched wt sample is >99%, meaning the SNP was introduced by editing\n",
    "    - strategy similar to Huang, X. et al. Programmable C-to-U RNA editing using the human APOBEC3A deaminase. The EMBO Journal 39, e104741 (2020). \n",
    "- Outputs\n",
    "    - a subdirectory for each condition containing tsvs of entries that passed each filter\n",
    "    - a tsv containing a count matrix of number of entries that passed each filter\n",
    "    - a tsv containing identity and %_snp data for each off-tgt hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# import statements #\n",
    "#####################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from Bio.Seq import Seq\n",
    "import pysam\n",
    "import math\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# User-Defined Variables #\n",
    "##########################\n",
    "# - define all variables below with paths to the required files\n",
    "# - this should be the only cell that requires modification\n",
    "\n",
    "# full path to compressed tsv from initial processing of the variant table\n",
    "tsv_path =''\n",
    "# full path to the sample map tsv, should contain 'sample', 'condition', and 'rep' columns mapping unique sample identifiers to a biological condidtion and biological replicate\n",
    "sample_map_path = ''\n",
    "# full path to reference genome gtf\n",
    "gtfgz_path = ''\n",
    "# full path to clinvar vcf\n",
    "clinvar_vcf_path=''\n",
    "\n",
    "# number of biological replicates\n",
    "replicates = 3\n",
    "# reference condition\n",
    "wt_condition = '01_transfection.control'\n",
    "# filter_strat = 'CURE-by-rep'\n",
    "\n",
    "# target snp to be analyzed in this notebook (ref, alt)\n",
    "target_snp = ('C','T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory 'off-tgt-analysis/{replicates}-reps' in the same directory as the variant table tsv\n",
    "out_dir = os.path.join(os.path.split(tsv_path)[0], 'off-tgt-analysis')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_dir = os.path.join(out_dir, f'{replicates}-reps')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# add a replicate column to the sample map, display the sample map for inspection\n",
    "sample_map_df = pd.read_csv(sample_map_path, sep='\\t')\n",
    "\n",
    "display(sample_map_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in var_df\n",
    "var_df = pd.read_csv(tsv_path, sep='\\t', compression='infer', low_memory=False)\n",
    "\n",
    "#print a list of all var_df columns and the head of var_df\n",
    "print(f'\\tcolumns:\\n')\n",
    "max_col_len = 0\n",
    "line = []\n",
    "for var_col in var_df.columns:\n",
    "    if len(var_col) > max_col_len:\n",
    "        max_col_len = len(var_col)\n",
    "for var_col in var_df.columns:\n",
    "    while len(var_col) < max_col_len:\n",
    "        var_col = var_col + ' '\n",
    "    line.append(var_col)\n",
    "    if len(line) > 2:\n",
    "        print(f'\\t{'\\t'.join(line)}')\n",
    "        line = []\n",
    "if len(line) > 0:\n",
    "    print(f'\\t{'\\t'.join(line)}')\n",
    "print(f'\\n')\n",
    "display(var_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_filter_list(var_df, sample_map, replicates, wt_condition, target_snp, filter_mode):\n",
    "def generate_filter_list(var_df, sample_map, replicates, wt_condition, target_snp):\n",
    "    ###################################################################################################\n",
    "    # Purpose: filter variant table for likely off targets, filtering strategy similar to CURE paper  #\n",
    "    #          and all filtering is done within matched replicates. Filtering summary:                #\n",
    "    #          1. var    - sample was called as a variant by HapplotypeCaller                         #\n",
    "    #          2. VOI    - sample variant matches the variant of interest                             #\n",
    "    #          3. DP     - read depth >= 20                                                           #\n",
    "    #          4. GQ     - depth by quality >=20 (corresponds to 99% confidence)                      #\n",
    "    #          5. non-wt - pct_ref in the matched wt sample is >99%, meaning the SNP was introduced   #\n",
    "    #                      by editing                                                                 #\n",
    "    # Inputs: 1. var_df - the annotated varaints table                                                #\n",
    "    #         2. sample_map - the dataframe matching sample identifiers to biological conditions      #\n",
    "    #         3. replicates - the number of replicates                                                #\n",
    "    #         4. wt_condition - the reference biological condition                                    #\n",
    "    #         5. target_snp - the snp being analyzed in this notebook                                 #\n",
    "    # Output: a list of filter names and a dictionary with the following structure, every sample id   #\n",
    "    #         in the map is a key:                                                                    #\n",
    "    #         sample_filters                                                                          #\n",
    "    #         |_ key: <sample_id>                                                                     #\n",
    "    #            |_value: dictionary of filter masks for <sample id>                                  #\n",
    "    #              |_key: <filter name>                                                               #\n",
    "    #                |_value: a filter mask for var_df matching <filter name>                         # \n",
    "    ###################################################################################################\n",
    "    \n",
    "    # generate list of all samples, will match columns of the var df\n",
    "    # sample_list = []\n",
    "    # for bio_condition in sample_map['condition'].unique():\n",
    "    #     for rep in range(1,replicates+1):\n",
    "    #         search_mask = sample_map['condition'] == bio_condition\n",
    "    #         sample_list.append((bio_condition, rep, sample_map.loc[search_mask, 'sample'].iloc[rep-1])) # list consists of tuples (bio_condition, rep, sample)\n",
    "\n",
    "    sample_filters = {}\n",
    "    # for bio_condition, rep, sample in sample_list:\n",
    "    for map_idx, map_row in sample_map_df.iterrows():\n",
    "        # bio_condition = map_row['condition']\n",
    "        rep = map_row['rep']\n",
    "        sample = map_row['sample']\n",
    "\n",
    "        sample_filters[sample] = {}\n",
    "        \n",
    "        # var filter components\n",
    "        filter_list = [\n",
    "            var_df[f'{sample}.GT'] != '0/0',\n",
    "            var_df[f'{sample}.GT'] != '0|0',\n",
    "            var_df[f'{sample}.GT'] != './.',\n",
    "            var_df[f'{sample}.GT'] != '.|.',\n",
    "            ~(var_df[f'{sample}.GT'].apply(pd.isna)),\n",
    "        ]\n",
    "        sample_filters[sample]['var'] = reduce(lambda x,y: x&y, filter_list)\n",
    "\n",
    "        # VOI filter components\n",
    "        filter_list = [\n",
    "            var_df[f'{sample}_ref'] == target_snp[0],\n",
    "            (var_df[f'{sample}_all_1'] == target_snp[1]) | (var_df[f'{sample}_all_2'] == target_snp[1])\n",
    "        ]\n",
    "        sample_filters[sample]['VOI'] = reduce(lambda x,y: x&y, filter_list)\n",
    "\n",
    "        # DP and GP filters\n",
    "        sample_filters[sample]['DP'] = var_df[f'{sample}.DP'] >= 20\n",
    "        sample_filters[sample]['GQ'] = var_df[f'{sample}.GQ'] >= 20\n",
    "\n",
    "        # non-wt filter\n",
    "\n",
    "        search_mask = sample_map['condition'] == wt_condition\n",
    "        search_mask = search_mask & (sample_map['rep'] == rep)\n",
    "        wt_sample = sample_map.loc[search_mask, 'sample'].iloc[0]\n",
    "        sample_filters[sample]['non-wt'] = var_df[f'{wt_sample}_pct_ref'] > 99\n",
    "\n",
    "    sample_filter_names = [\n",
    "        'var',\n",
    "        'VOI',\n",
    "        'DP',\n",
    "        'GQ',\n",
    "        'non-wt',\n",
    "    ]\n",
    "\n",
    "    return sample_filters , sample_filter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f640fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build summary table showing number of entries passing each sucessive filter\n",
    "# sample_filters_dict, sample_filter_names = generate_filter_list(var_df=var_df, sample_map=sample_map_df, replicates=replicates, wt_condition=wt_condition, target_snp=target_snp, filter_mode=filter_strat)\n",
    "sample_filters_dict, sample_filter_names = generate_filter_list(var_df=var_df, sample_map=sample_map_df, replicates=replicates, wt_condition=wt_condition, target_snp=target_snp)\n",
    "\n",
    "filter_counts_df = pd.DataFrame()\n",
    "\n",
    "# add 'subset' column to filter counts_df that name every filtered subset\n",
    "subset_list = ['total_entries']\n",
    "for bio_condition in sample_map_df['condition'].unique():\n",
    "    total_filt_name = []\n",
    "    for filt_name in sample_filter_names:\n",
    "        total_filt_name.append(filt_name)\n",
    "        subset_list.append(f'{bio_condition}_{'_'.join(total_filt_name)}')\n",
    "filter_counts_df['subset'] = subset_list\n",
    "\n",
    "# for each biological replicate tabulate the hits after each filter then add the column of counts to filter_counts_df\n",
    "for rep in range(1, replicates + 1):\n",
    "    count_col_list = []\n",
    "    count_col_list.append(len(var_df))\n",
    "\n",
    "    for bio_condition in sample_map_df['condition'].unique():\n",
    "        sample_mask = sample_map_df['condition'] == bio_condition\n",
    "        sample = sample_map_df.loc[sample_mask, 'sample'].iloc[rep-1]\n",
    "        sample_filters = sample_filters_dict[sample]\n",
    "        \n",
    "        total_filter = []\n",
    "        for filt_name, filt_mask in sample_filters.items():\n",
    "            total_filter.append(filt_mask)\n",
    "            count_col_list.append(len(var_df[reduce(lambda x,y: x&y, total_filter)]))\n",
    "\n",
    "    filter_counts_df[f'r{rep}'] = count_col_list\n",
    "\n",
    "# the last column in filter_counts_df will hold counts of var_df entries where all three replicates passed a given filter\n",
    "count_col_list = []\n",
    "count_col_list.append(len(var_df))\n",
    "\n",
    "rep_count_df = pd.DataFrame()\n",
    "rep_passing_list = list(range(1,replicates + 1))\n",
    "rep_count_df['replicates_passed'] = rep_passing_list\n",
    "\n",
    "for bio_condition in sample_map_df['condition'].unique():\n",
    "    # iterating through indices of the filter list i indicates the index of the last filter that should be combined to get the next filter step\n",
    "    for i in range(len(sample_filter_names)):\n",
    "        # will store the combined filters for each of the three replicates for the given condition\n",
    "        rep_filters = []\n",
    "        filter_name = []\n",
    "        for rep in range(1, replicates + 1):\n",
    "            # combines the names of all filters being applied\n",
    "            filter_name = [key for key in list(sample_filters.keys())[:i+1]]\n",
    "            filter_name = '_'.join(filter_name)\n",
    "\n",
    "            # determines the relevant sample for the given biological condition and biological replicate\n",
    "            sample_mask = sample_map_df['condition'] == bio_condition\n",
    "            sample_mask = sample_mask & (sample_map_df['rep'] == rep)\n",
    "            sample = sample_map_df.loc[sample_mask, 'sample'].iloc[0]\n",
    "\n",
    "            # dictionary of filters for the sample\n",
    "            sample_filters = sample_filters_dict[sample]\n",
    "            # combined filter for the sample and filtering step i is added to rep_filters\n",
    "            sample_filter = [sample_filters[key] for key in list(sample_filters.keys())[:i+1]]\n",
    "            rep_filters.append(reduce(lambda x,y: x&y, sample_filter))\n",
    "\n",
    "        # add a count of how many entries passed all replicate filters\n",
    "        count_col_list.append(len(var_df[reduce(lambda x,y: x&y, rep_filters)]))\n",
    "        \n",
    "filter_counts_df['intersection'] = count_col_list\n",
    "display(filter_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each biological condition and each filter output a compressed tsv of all entries passing the filter these will be grouped into subdirectories for each condition\n",
    "# the count matrix will be output as a tsv in the top level output directory\n",
    "\n",
    "for bio_condition in sample_map_df['condition'].unique():    \n",
    "    print(f'{bio_condition}:\\n')\n",
    "\n",
    "    bio_condition_out_dir = os.path.join(out_dir, bio_condition)\n",
    "    os.makedirs(bio_condition_out_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(1, len(sample_filter_names) + 1):\n",
    "        filter_names_list = sample_filter_names[:i]\n",
    "        combined_filter_name = '_'.join(filter_names_list)\n",
    "\n",
    "        rep_filters = []\n",
    "        for rep in range(1,replicates + 1):\n",
    "            sample_mask = sample_map_df['condition'] == bio_condition\n",
    "            sample = sample_map_df.loc[sample_mask, 'sample'].iloc[rep-1]\n",
    "            sample_filter = []\n",
    "            for filter_name in filter_names_list:\n",
    "                sample_filter.append(sample_filters_dict[sample][filter_name])\n",
    "            rep_filters.append(reduce(lambda x,y: x&y, sample_filter))\n",
    "        \n",
    "        combined_filter = reduce(lambda x,y: x&y, rep_filters)\n",
    "\n",
    "        if len(var_df[combined_filter]) > 0:\n",
    "            out_tsv_path = os.path.join(bio_condition_out_dir, f'{bio_condition}-{combined_filter_name}.tsv.gz')\n",
    "            var_df[combined_filter].to_csv(out_tsv_path, sep='\\t', float_format='%.2f', compression='gzip')\n",
    "\n",
    "        print(f'\\tFilter:     \\t{combined_filter_name}')\n",
    "        print(f'\\ttsv entries:\\t{len(var_df[combined_filter])}\\n')\n",
    "out_tsv_path = os.path.join(out_dir, 'filter-counts.tsv')\n",
    "filter_counts_df.to_csv(out_tsv_path, sep='\\t', index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0654a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output a tsv containing each of the final off-tgt hits and the mean pct_snp for each non wt condition to the output directory\n",
    "\n",
    "merge_cols = ['chrom', 'pos', 'ref', 'alt', 'gene_name']\n",
    "final_hits_df = pd.DataFrame(columns=merge_cols)\n",
    "final_filt_name = '_'.join(sample_filter_names)\n",
    "\n",
    "for bio_condition in sample_map_df['condition'].unique():\n",
    "    if bio_condition != wt_condition:\n",
    "        rep_filter_list = []\n",
    "        rep_sample_list = []\n",
    "        for rep in range(1,replicates + 1):\n",
    "            sample_mask = sample_map_df['condition'] == bio_condition\n",
    "            sample = sample_map_df.loc[sample_mask, 'sample'].iloc[rep-1]\n",
    "            rep_sample_list.append(sample)\n",
    "\n",
    "            sample_filter_list = []\n",
    "            for filter_name in sample_filter_names:\n",
    "                sample_filter_list.append(sample_filters_dict[sample][filter_name])\n",
    "\n",
    "            rep_filter_list.append(reduce(lambda x,y: x&y, sample_filter_list))\n",
    "\n",
    "        combined_filter = reduce(lambda x,y: x&y, rep_filter_list)\n",
    "\n",
    "        pct_snp_col_list = [f'{sample}_pct_snp' for sample in rep_sample_list]\n",
    "        merge_df = var_df.loc[combined_filter, merge_cols]\n",
    "        mean_pct_snp_col = pd.Series()\n",
    "        \n",
    "        for pct_snp_col in pct_snp_col_list:\n",
    "            merge_df[pct_snp_col] = var_df.loc[combined_filter, pct_snp_col]\n",
    "\n",
    "            if len(mean_pct_snp_col) == 0:\n",
    "                mean_pct_snp_col = var_df.loc[combined_filter, pct_snp_col]\n",
    "            else:\n",
    "                mean_pct_snp_col = mean_pct_snp_col + var_df.loc[combined_filter, pct_snp_col]\n",
    "        \n",
    "        mean_pct_snp_col = mean_pct_snp_col.apply(lambda x: x/len(rep_sample_list))\n",
    "\n",
    "        merge_df[f'{bio_condition.split('_')[1]}'] = mean_pct_snp_col\n",
    "\n",
    "        final_hits_df = pd.merge(left=final_hits_df, right=merge_df, on=merge_cols, how='outer')\n",
    "\n",
    "final_hits_df['gene_name'] = final_hits_df['gene_name'].apply(lambda x: ','.join(set(x.split(','))) if not pd.isna(x) else '')\n",
    "\n",
    "mask = sample_map_df['condition'] != wt_condition\n",
    "out_col_order = merge_cols + [f'{sample}_pct_snp' for sample in list(sample_map_df.loc[mask, 'sample'])] + [bio_condition.split('_')[1] for bio_condition in list(sample_map_df.loc[mask, 'condition'].unique())]\n",
    "\n",
    "out_tsv_path = os.path.join(out_dir, 'final-hit-mean-pct-snps.tsv')\n",
    "final_hits_df[out_col_order].to_csv(out_tsv_path, sep='\\t', index=True, float_format='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
